# Pipeline
1. 从语音中提取与用户无关的特征
2. 用语音特征驱动动画，使用**参数进行控制**
3. 对于新用户，提供一段时间的讲话视频，生成对应的控制参数

## 语音特征
### Formant特征
- 使用source-filter模型可以提取出formant的估计，formant与发音内容有着很大关系
- 但是不同人的Formant也会有所区别，比如口音带来的影响

### Voice Feature Converter
- 在训练数据中选择一人作为标准
- 使用深度网络进行语音特征的转换，将所有其他人的语音特征都转换到**标准人**身上
- 所有的语音特征，需要经过这个网络，首先转换成**标准特征**

## 参数控制
- 值得注意的是风格产生往往体现在，发音时会有不同的面部动作
	- 比如不说话时。有人喜欢闭着嘴，有人会微微张开，有人会抿着嘴
	- 发/b/音时。多数人会直接抿嘴，有的人会下唇覆盖上唇
	- 发/i/音时。有人会咧嘴，有人则不会
- 这些特点之间可以相互组合而存在，也有程度大小之分。即控制参数
	- 维度较高。用于区分各种发音情况下的习惯
	- 可插值（希望能够满足）。
- 考虑使用GAN中的noise作为控制参数

### 语音到风格动画
- 直接在语音特征到动画的过程中加入参数控制
- 端对端的方案下，无需特殊处理中间数据，但是训练难度较大

### 语音到中性动画，再到风格动画
- 首先用语音生成中性的动画。确保动画的基本质量
- 再用参数来控制从中性动画到风格动画的过程
- 分步训练更容易
	- 首先单人数据的语音到动画，已经有很多论文尝试，效果也不错
	- 在有中性动画的基础上，训练真实、优秀的生成器，可能更容易

## 参数生成
- 因为参数经过神经网络的高度非线性变化，很难直观地根据动画结果的差距来调整参数
- 随机产生一个控制参数，将网络固定，用新用户的少量数据进行fine-tune，反向传播，修改控制参数
- 如果控制参数有足够强的控制能力，应该能够产生不错的结果